{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM DETECTION OF YOU-TUBE COMMENTS USING MULTI LAYER PERCEPTRON\n",
    "### The necessary imports to perform preprocessing and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading all Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfPsy = pd.read_csv(r\"YouTube-Spam-Collection-v1/Youtube01-Psy.csv\")\n",
    "dfKatyPerry = pd.read_csv(r\"YouTube-Spam-Collection-v1/Youtube02-KatyPerry.csv\")\n",
    "dfLMFAO = pd.read_csv(r\"YouTube-Spam-Collection-v1/Youtube03-LMFAO.csv\")\n",
    "dfEminem = pd.read_csv(r\"YouTube-Spam-Collection-v1/Youtube04-Eminem.csv\")\n",
    "dfShakira = pd.read_csv(r\"YouTube-Spam-Collection-v1/Youtube05-Shakira.csv\")\n",
    "\n",
    "# Concatinating all the datasets to a single file.\n",
    "df = pd.concat([dfPsy, dfKatyPerry, dfLMFAO, dfEminem, dfShakira])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing tokenizer and lemmatizer and writing a function for the preprocessing task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tokenizer = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(col):\n",
    "    '''\n",
    "    Preprocessing includes: converting to lowercase, removing punctuation, tokenizing and lemmatizing\n",
    "    input: text row of dataframe\n",
    "    output: list with lemmatized words\n",
    "    '''\n",
    "    col = col.lower()\n",
    "    col = [char for char in col if char not in punctuation]\n",
    "    #rejoin the characters after removing punctuation\n",
    "    col = ''.join(col)\n",
    "    #tokenize and add pos tag so lemmatizer doesn't see all words as nouns\n",
    "    col = nltk.pos_tag(w_tokenizer.tokenize(col))\n",
    "    #lemmatize with WordNetLemmatizer\n",
    "    return \" \".join([lemmatizer.lemmatize(word, tag[0]) if tag[0] in ['a', 'r', 'n', 'v'] else word for word, tag in col])\n",
    "#return [lemmatizer.lemmatize(word, tag[0].lower()) if tag[0].lower() in ['a', 'r', 'n', 'v'] else word for word, tag in col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying preprocess on the the Content Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CONTENT\"]=df[\"CONTENT\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the train-test split on the dataset (70/30 split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[\"CONTENT\"]\n",
    "Y = df[\"CLASS\"]\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = .3,random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the Vectorizer on the Content Column to convert the sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\",max_features=1000)\n",
    "\n",
    "xtr = tfidf.fit_transform(X_train)\n",
    "xts = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Multi Layer Perceptron Characteristics Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "class Perceptron(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptron, self).__init__()\n",
    "        self.fc = torch.nn.Linear(1,1)\n",
    "        self.relu = torch.nn.ReLU() # instead of Heaviside step fn\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = self.relu(x) # instead of Heaviside step fn\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Feedforward functionality of the MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward(torch.nn.Module):\n",
    "        def __init__(self, input_size, hidden_size):\n",
    "            super(Feedforward, self).__init__()\n",
    "            self.input_size = input_size\n",
    "            self.hidden_size  = hidden_size\n",
    "            self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "            self.relu = torch.nn.ReLU()\n",
    "            self.fc2 = torch.nn.Linear(self.hidden_size, 1)\n",
    "            self.sigmoid = torch.nn.Sigmoid()\n",
    "        def forward(self, x):\n",
    "            hidden = self.fc1(x)\n",
    "            relu = self.relu(hidden)\n",
    "            output = self.fc2(relu)\n",
    "            output = self.sigmoid(output)\n",
    "            return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split and converting it to floattensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrain = torch.FloatTensor(xtr.todense()).float()\n",
    "Ytrain = torch.FloatTensor(np.array(Y_train)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtest = torch.FloatTensor(xts.todense()).float()\n",
    "Ytest = torch.FloatTensor(np.array(Y_test)).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Feedforward(1000,10)\n",
    "Ytrain = Ytrain.unsqueeze(1).float()\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the Model and Calculating the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: train loss: 0.42702144384384155\n",
      "Epoch 1: train loss: 0.40937769412994385\n",
      "Epoch 2: train loss: 0.3919745981693268\n",
      "Epoch 3: train loss: 0.3749280869960785\n",
      "Epoch 4: train loss: 0.35832270979881287\n",
      "Epoch 5: train loss: 0.34220466017723083\n",
      "Epoch 6: train loss: 0.3266586661338806\n",
      "Epoch 7: train loss: 0.31173065304756165\n",
      "Epoch 8: train loss: 0.29746970534324646\n",
      "Epoch 9: train loss: 0.2838950753211975\n",
      "Epoch 10: train loss: 0.2710270285606384\n",
      "Epoch 11: train loss: 0.25887617468833923\n",
      "Epoch 12: train loss: 0.24743963778018951\n",
      "Epoch 13: train loss: 0.2367090880870819\n",
      "Epoch 14: train loss: 0.22666020691394806\n",
      "Epoch 15: train loss: 0.2172727882862091\n",
      "Epoch 16: train loss: 0.20852504670619965\n",
      "Epoch 17: train loss: 0.20038393139839172\n",
      "Epoch 18: train loss: 0.19281437993049622\n",
      "Epoch 19: train loss: 0.18578431010246277\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "epoch = 20\n",
    "for epoch in range(epoch):\n",
    "    optimizer.zero_grad()\n",
    "    # y_pred = y_pred.squeeze(1)\n",
    "    # Forward pass\n",
    "    y_pred = model(Xtrain)\n",
    "    # Compute Loss\n",
    "    loss = criterion(y_pred, Ytrain)\n",
    "\n",
    "   \n",
    "    print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "    pred = torch.max(y_pred, 1)[1].eq(Ytrain).sum()\n",
    "    # print (\"Accuracy is: \", (pred/len(Xtrain)).item())\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "### https://medium.com/biaslyai/pytorch-introduction-to-neural-network-feedforward-neural-network-model-e7231cff47cb\n",
    "\n",
    "### https://discuss.pytorch.org/t/runtimeerror-multi-target-not-supported-newbie/10216/11\n",
    "\n",
    "### https://stackoverflow.com/questions/67845882/indexerror-target-1-is-out-of-bounds\n",
    "\n",
    "### https://pytorch.org/text/stable/data_metrics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "082e9a3bcad0a290d0001e938aa60b99250c6c2ef33a923c00b70f9826caf4b7"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
